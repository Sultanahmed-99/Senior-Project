{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25148,
     "status": "ok",
     "timestamp": 1672535805487,
     "user": {
      "displayName": "Sultan AL-yami",
      "userId": "10209379315580867820"
     },
     "user_tz": -180
    },
    "id": "GPYEAZwB5op1",
    "outputId": "63e91c1b-0ed1-4cf3-b600-37b0a4e31360"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hN7LCKl-5soe"
   },
   "outputs": [],
   "source": [
    "# import cv2 as cv\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import mediapipe\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTES5jWCAaal"
   },
   "source": [
    "* Module For loading faces from files then extract faces also resize images \n",
    "--> in which face net model recive images with targetsize ->> (160, 160) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZoU6AV5AJdy"
   },
   "source": [
    "**EXTRACTING Faces embdinges** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PTljdsvR79_E"
   },
   "outputs": [],
   "source": [
    "# from keras_facenet import FaceNet\n",
    "# embedder = FaceNet()\n",
    "\n",
    "# def get_embedding(face_img):\n",
    "#     face_img = face_img.astype('float32') # 3D(160x160x3)\n",
    "#     face_img = np.expand_dims(face_img, axis=0) \n",
    "#     # 4D (Nonex160x160x3)\n",
    "#     yhat= embedder.embeddings(face_img)\n",
    "#     return yhat[0] # 512D image (1x1x512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle  \n",
    "# # import cv2 as cv\n",
    "# # import os\n",
    "# # import numpy as np\n",
    "# # import tensorflow as tf\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # import mediapipe\n",
    "# pickle_modle = pickle.load(open(\"model_2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sly', 'sly', 'sly', 'sly', 'sly', 'sly', 'sly', 'sly', 'sly',\n",
       "       'sly', 'abd', 'abd', 'abd', 'abd', 'abd', 'cr7', 'cr7', 'cr7',\n",
       "       'cr7', 'cr7', 'cr7', 'messi', 'messi', 'messi', 'messi', 'messi',\n",
       "       'messi', 'messi'], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # y = np.load(\"faces_embeddings_done_4classes.npz\") \n",
    "# nn = np.load(\"faces_embeddings_done_4classes.npz\")\n",
    "# # npzfile.files\n",
    "# y = nn['arr_1']\n",
    "# y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(y)\n",
    "# y = encoder.transform(y)\n",
    "# y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "error",
     "timestamp": 1672540646530,
     "user": {
      "displayName": "Sultan AL-yami",
      "userId": "10209379315580867820"
     },
     "user_tz": -180
    },
    "id": "j0WN6X8FBEh6",
    "outputId": "3df574bd-a419-4582-80ab-c260d84d095f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46744\\611251776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                        \u001b[1;32mif\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mypreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mypreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontFace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_TRIPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontScale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthickness\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "import mediapipe \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "from keras_facenet import FaceNet\n",
    "embedder = FaceNet()\n",
    "\n",
    "def get_embedding(face_img):\n",
    "    face_img = face_img.astype('float32') \n",
    "    face_img = np.expand_dims(face_img, axis=0) \n",
    "    yhat= embedder.embeddings(face_img)\n",
    "    return yhat[0] \n",
    "\n",
    "\n",
    "import pickle  \n",
    "pickle_modle = pickle.load(open(\"model_2.pkl\", \"rb\"))\n",
    "\n",
    "nn = np.load(\"faces_embeddings_done_4classes.npz\")\n",
    "y = nn['arr_1']\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "mediapipe_drawing = mediapipe.solutions.drawing_utils\n",
    "mediapipe_facemesh = mediapipe.solutions.face_mesh\n",
    "face_mesh = mediapipe_facemesh.FaceMesh()\n",
    "face_detector_model = mediapipe.solutions.face_detection\n",
    "face_detector = face_detector_model.FaceDetection(min_detection_confidence=0.6)\n",
    "draw_specs = mediapipe_drawing.DrawingSpec(thickness = 1 ,circle_radius = 2 )\n",
    "\n",
    "\n",
    "save_dir = \"Desktop/images/\"\n",
    "image_counter = 0 \n",
    "\n",
    "while True :\n",
    "    timer = cv2.getTickCount()\n",
    "    sucsses , img = cam.read()\n",
    "    image_RGB = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n",
    "    RESULT = face_mesh.process(image_RGB)\n",
    "    reusult2 = face_detector.process(image_RGB) \n",
    "    if RESULT.multi_face_landmarks:\n",
    "        if reusult2.detections:\n",
    "            \n",
    "            for facelanmark  in RESULT.multi_face_landmarks:\n",
    "                \n",
    "                \n",
    "\n",
    "                for id,lm in enumerate(facelanmark.landmark):\n",
    "                    ih , iw , ic = img.shape\n",
    "                    x , y = (int(lm.x*iw) , int(lm.y*ih))\n",
    "                for face_detected in reusult2.detections:\n",
    "                       ih , iw , ic = img.shape\n",
    "                       bounding_box = face_detected.location_data.relative_bounding_box\n",
    "                       \n",
    "\n",
    "                       x = int(bounding_box.xmin * iw)\n",
    "                       w = int(bounding_box.width * iw)\n",
    "                       y = int(bounding_box.ymin * ih)\n",
    "                       h = int(bounding_box.height * ih)\n",
    "                       \n",
    "#cv2.rectangle(img,(x,y),(x +w , y+h ),color = (255,0,0),thickness = 3 )\n",
    "                       CROPD_IMG = img[y:y+h , x:x+w]\n",
    "                       test_im = get_embedding(CROPD_IMG)\n",
    "\n",
    "                       image = cv2.rectangle(img,(x,y),(x +w , y+h ),color = (255,0,0),thickness = 3 )\n",
    "                       test_im = [test_im]\n",
    "#                        t_im = cv2.resize(t_im, (160,160))\n",
    "                       ypreds = pickle_modle.predict(test_im)\n",
    "                       \n",
    "                        \n",
    "                       if encoder.classes_[ypreds[0]] in y:\n",
    "                            name = encoder.classes_[ypreds[0]]\n",
    "                            cv2.putText(img=image, text=name, org=(x, y-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=3, color=(0, 255, 0),thickness=1)\n",
    "                       else:\n",
    "                            cv2.putText(img=image, text=\"Person not known\", org=(x, y-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=3, color=(0, 0, 255),thickness=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#                        image = cv2.rectangle(img,(x,y),(x +w , y+h ),color = (255,0,0),thickness = 3 )\n",
    "#                        cv2.putText(img=image, text=encoder.inverse_transform(ypreds)[0], org=(x, y-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=3, color=(0, 255, 0),thickness=1)\n",
    "    \n",
    "         \n",
    "    \n",
    "#                        if image_counter == 2000 : \n",
    "#                             break\n",
    "        \n",
    "#                        cv2.imwrite(save_dir + \"crop_img_{}.png\".format(image_counter), CROPD_IMG)\n",
    "#                        image_counter +=1\n",
    "                        \n",
    "                        \n",
    "\n",
    "    #             another for loop to get number of all land markas to get deep \n",
    "    #             knowing start and edges of land marks \n",
    "            \n",
    "                \n",
    "#                 cropd_images = img[y:y+ih - 500  , x:x+iw-500]\n",
    "                \n",
    "#                 furure more need to save all in a list \n",
    "#                 which will be done in a module will be much faster \n",
    "                \n",
    "    # calculating frames \n",
    "#     Ctime = time.time()\n",
    "    # fps \n",
    "    # current_time - previos_time \n",
    "#     fps = 1/(Ctime - Ptime)\n",
    "    fps = cv2.getTickFrequency()/(cv2.getTickCount()-timer)\n",
    "#     Ptime = Ctime\n",
    "    cv2.putText(img =image, text=encoder.inverse_transform(ypreds)[0], org=(x, y-10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0, 0, 255),thickness=2)\n",
    "   \n",
    "    # SHOWING IMAGE FROM CAPTURED CAM \n",
    "    cv2.imshow('ORGINAL IMAGE' , img) \n",
    "    cv2.imshow('croped IMAGE' , CROPD_IMG)  \n",
    "#     cv2.imshow('crop IMAGE' , cropd_images)\n",
    "    # ESC | EXIT FROM THE FRAME SHOUTDOWN CAM \n",
    "#     if cv2.waitKey(1) == 27:\n",
    "#         break \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "         break\n",
    "   \n",
    "cam.release()\n",
    "cv2.destoryAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiqA0awuLjfzSHZTIv+07j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
